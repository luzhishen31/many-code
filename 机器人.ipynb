{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20a34985-99fb-4972-bb15-3a85c170be2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-9jhhtqy0ZreywviRZH8WdFAshIyvY', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='你好！我是一个人工智能聊天机器人，可以帮助你回答问题、提供信息、进行对话和娱乐等。如果你有任何问题', role='assistant', function_call=None, tool_calls=None))], created=1720679797, model='gpt-4', object='chat.completion', service_tier=None, system_fingerprint='fp_811936bd4f', usage=CompletionUsage(completion_tokens=50, prompt_tokens=25, total_tokens=75))\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import ujson as json\n",
    "from openai import OpenAI\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"3bc24a63-eb36-43b3-ac21-c944005cf008\"\n",
    "# url无需添加具体接口的后缀，openai的sdk会自动补全\n",
    "# 若是通过http方式调用则需要完整的接口url\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"http://gpt-proxy.jd.com/gateway/azure\"\n",
    "\n",
    "\n",
    "def open_ai_sdk():\n",
    "    client = OpenAI(\n",
    "        api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "        base_url=os.environ[\"OPENAI_API_BASE\"],\n",
    "    )\n",
    "    # 此处传入headers中的Authorization 与在client传api_key是一样的\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {os.environ['OPENAI_API_KEY']}\"\n",
    "    }\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"你是一个聊天机器人\"},\n",
    "        {\"role\": \"user\", \"content\": \"你是谁？\"}\n",
    "    ]\n",
    "    # 本示例为请求聊天完成接口，如果需要请求别的接口请修改\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4-1106-preview\",  # 模型名称\n",
    "        messages=messages,         # 聊天上下文\n",
    "        temperature=0.5,           # 模型响应的温度参数\n",
    "        max_tokens=50,             # 最大响应token数\n",
    "        top_p=1,                   # top-p参数\n",
    "        frequency_penalty=0,       # 频率惩罚\n",
    "        presence_penalty=0,        # 存在惩罚\n",
    "        stream=False               # 是否流式返回结果\n",
    "        # 入参时erp改为不必填 但如果输入了erp会校验erp是否真实存在，erp仅用于观测实际调用人\n",
    "        # extra_body={\"erp\": \"python\"}\n",
    "        # 请求头\n",
    "        # extra_headers=headers\n",
    "\n",
    "    )\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(open_ai_sdk())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0c6ec253-bbd3-4763-b32b-1d18a30b27bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import logging\n",
    "\n",
    "# 设置日志记录\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 从环境变量中获取API密钥和基础URL\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"3bc24a63-eb36-43b3-ac21-c944005cf008\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"http://gpt-proxy.jd.com/gateway/azure\"\n",
    "\n",
    "def open_ai_sdk(model_name, messages, temperature=0.5, max_tokens=1000):\n",
    "    try:\n",
    "        client = OpenAI(\n",
    "            api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "            base_url=os.environ[\"OPENAI_API_BASE\"],\n",
    "        )\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stream=False\n",
    "        )\n",
    "        logger.info(\"API response received.\")\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0a93fc07-8468-444a-9fb1-4e683705a93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://gpt-proxy.jd.com/gateway/azure/chat/completions \"HTTP/1.1 200 \"\n",
      "INFO:__main__:API response received.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "聊天机器人的回复： LDA全称为“Latent Dirichlet Allocation”，是一种常用于文本数据的主题模型。它能够从文档集合中自动地发现主题，并将文档表示为这些主题的混合。LDA背后的基本思想是，文档是由隐含的主题分布生成的，而每个主题又是由一系列单词的分布构成的。\n",
      "\n",
      "具体来说，LDA模型假设：\n",
      "\n",
      "1. **每个文档表示为隐含主题的混合**。每个文档都可以看作是多个主题的集合，但每个主题在不同文档中的比例不尽相同。\n",
      "2. **每个主题表示为单词的分布**。每个主题都可以通过它包含的单词及其概率来描述，不同主题包含不同的单词集合。\n",
      "\n",
      "LDA的工作流程大致可以分为以下几个步骤：\n",
      "\n",
      "1. **确定主题数量**：在运行LDA模型之前，需要预先设定要提取的主题数量。这个数量可以根据实际需要和数据集的大小来确定。\n",
      "2. **随机分配**：一开始，LDA会随机给文档中的每个单词分配一个主题。\n",
      "3. **迭代更新**：通过迭代过程，LDA逐渐调整单词与主题之间的分配，以便最大化模型的联合概率。这个过程通常使用吉布斯采样（Gibbs sampling）或变分贝叶斯推断（Variational Bayes inference）等技术。\n",
      "4. **得到结果**：经过足够的迭代后，每个文档的主题分布和每个主题的单词分布逐渐稳定。这时，可以根据这些分布来分析文档的主题结构。\n",
      "\n",
      "LDA模型在文本挖掘、自然语言处理等领域有广泛的应用，如文档分类、信息检索、情感分析等。通过LDA，我们可以更好地理解和解释大规模文档集合的内容和结构。\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    new_messages = [\n",
    "        {\"role\": \"system\", \"content\": \"你是一个帮助者。\"},\n",
    "        {\"role\": \"user\", \"content\": \"我想问一下LDA是什么？\"},\n",
    "        # 添加一个中文提示\n",
    "        {\"role\": \"assistant\", \"content\": \"LDA（潜在狄利克雷分配）是一种统计模型，用于发现文档集合中的主题。\"}\n",
    "    ]\n",
    "    response = open_ai_sdk(\"gpt-4-1106-preview\", new_messages)\n",
    "    if response:\n",
    "        try:\n",
    "            # 假设 ChatCompletion 对象有一个 .data 属性或 .to_dict() 方法来获取响应数据\n",
    "            # 根据实际情况修改以下代码\n",
    "            if hasattr(response, 'data'):\n",
    "                response_data = response.data\n",
    "            elif hasattr(response, 'to_dict'):\n",
    "                response_data = response.to_dict()\n",
    "            else:\n",
    "                # 如果这两个属性/方法都不存在，你可能需要根据实际情况来调整\n",
    "                raise AttributeError(\"ChatCompletion 对象没有可访问的数据属性或方法\")\n",
    "\n",
    "            # 输出聊天机器人的回复内容\n",
    "            assistant_reply = response_data[\"choices\"][0][\"message\"][\"content\"]\n",
    "            print(\"聊天机器人的回复：\", assistant_reply)\n",
    "        except AttributeError as e:\n",
    "            print(\"访问响应数据时出错：\", e)\n",
    "        except KeyError as e:\n",
    "            print(\"响应数据中的键错误：\", e)\n",
    "    else:\n",
    "        print(\"未能从 OpenAI API 获取有效响应。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54268a7a-e65b-4eb9-a740-61775ffbc316",
   "metadata": {},
   "source": [
    "## 模拟rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dda7738e-3f1b-46f3-857b-c9f4674a13c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Response generated successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated response: This is a generated response based on the input messages and context.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "# 假设的检索系统\n",
    "class RetrievalSystem:\n",
    "    def search(self, query):\n",
    "        # 这里应该是检索相关信息的逻辑\n",
    "        # 返回一个假设的文档列表\n",
    "        return [{\"content\": \"This is a retrieved document related to the query.\"}]\n",
    "\n",
    "# 假设的语言模型\n",
    "class LanguageModel:\n",
    "    def generate(self, messages, context):\n",
    "        # 这里应该是基于消息和上下文生成回应的逻辑\n",
    "        # 返回一个假设的生成回应\n",
    "        return \"This is a generated response based on the input messages and context.\"\n",
    "\n",
    "# 假设的 RAG SDK\n",
    "class RAGSDK:\n",
    "    def __init__(self, retrieval_system, language_model):\n",
    "        self.retrieval_system = retrieval_system\n",
    "        self.language_model = language_model\n",
    "\n",
    "    def generate_response(self, model_name, messages, temperature=0.5, max_tokens=1000):\n",
    "        # 假设逻辑来处理消息，检索信息，并生成回应\n",
    "        query = messages[-1]['content']  # 假设最后一条消息是用户查询\n",
    "        context = self.retrieval_system.search(query)\n",
    "        response = self.language_model.generate(messages, context)\n",
    "        return response\n",
    "\n",
    "# 以下是使用 RAGSDK 的示例代码\n",
    "def rag_sdk(model_name, messages, temperature=0.5, max_tokens=1000):\n",
    "    try:\n",
    "        # 初始化检索系统和语言模型\n",
    "        retrieval_system = RetrievalSystem()\n",
    "        language_model = LanguageModel()\n",
    "        rag_sdk_instance = RAGSDK(retrieval_system, language_model)\n",
    "\n",
    "        # 生成响应\n",
    "        response = rag_sdk_instance.generate_response(model_name, messages, temperature, max_tokens)\n",
    "        logging.info(\"Response generated successfully.\")\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# 示例用法\n",
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    # 消息列表\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is LDA?\"}\n",
    "    ]\n",
    "\n",
    "    # 调用函数\n",
    "    reply = rag_sdk(\"rag-model\", messages)\n",
    "    if reply:\n",
    "        print(\"Generated response:\", reply)\n",
    "    else:\n",
    "        print(\"Failed to generate a response.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7815dde-4ca8-4ab1-be45-870822dd6ac5",
   "metadata": {},
   "source": [
    "## 转向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c4e0f01a-4e29-4d99-8b52-3285edfa1cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1:\n",
      "Most similar question: 什么是数据可视化？\n",
      "Answer: 数据可视化是将数据转换为图形或图表形式的过程，以便更容易分析和理解。\n",
      "Distance: 2.17663448362023\n",
      "\n",
      "Rank 2:\n",
      "Most similar question: 如何使用卷积神经网络(CNN)处理图像？\n",
      "Answer: CNN是一种特殊的神经网络，通过卷积层提取图像特征，用于图像处理任务。\n",
      "Distance: 2.968803898730111\n",
      "\n",
      "Rank 3:\n",
      "Most similar question: 什么是过拟合？\n",
      "Answer: 过拟合是指模型在训练数据上学习得太好，导致泛化能力差。\n",
      "Distance: 3.002709095996682\n",
      "\n",
      "Rank 4:\n",
      "Most similar question: 什么是机器学习？\n",
      "Answer: 机器学习是使计算机能够从数据中学习并做出预测或决策的学科领域，而无需每一步都进行明确编程。\n",
      "Distance: 3.031502297215975\n",
      "\n",
      "Rank 5:\n",
      "Most similar question: 如何在pandas中处理缺失数据？\n",
      "Answer: 可以使用dropna()删除或fillna()填充缺失数据。\n",
      "Distance: 3.052775521979715\n",
      "\n",
      "Rank 6:\n",
      "Most similar question: 什么是Transformer模型？\n",
      "Answer: Transformer模型是一种基于自注意力机制的深度学习模型，常用于NLP任务。\n",
      "Distance: 3.1214053680562994\n",
      "\n",
      "Rank 7:\n",
      "Most similar question: 如何在Sklearn中实现KNN算法？\n",
      "Answer: 可以使用sklearn.neighbors中的KNeighborsClassifier类来实现KNN（k最近邻）算法。\n",
      "Distance: 3.132223478206783\n",
      "\n",
      "Rank 8:\n",
      "Most similar question: 什么自然语言处理(NLP)？\n",
      "Answer: 自然语言处理是计算机科学、人工智能和语言学领域的一个分支，用于处理人类语言。\n",
      "Distance: 3.1586851536071765\n",
      "\n",
      "Rank 9:\n",
      "Most similar question: 如何在Python中安装一个库？\n",
      "Answer: 可以使用pip包管理器通过命令行（如 pip install library-name）来安装。\n",
      "Distance: 3.308638170948072\n",
      "\n",
      "Rank 10:\n",
      "Most similar question: 如何在Python中读取CSV文件？\n",
      "Answer: 可以使用pandas库中的read_csv函数来读取CSV文件。\n",
      "Distance: 3.490525867202296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# 初始化模型和分词器\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "    cls_embedding = last_hidden_state[:, 0, :]\n",
    "    return cls_embedding.detach().numpy()\n",
    "\n",
    "# 读取CSV文件中的问题和答案\n",
    "csv_file_path = '问题集.csv'  # 替换为你的CSV文件路径\n",
    "df = pd.read_csv(csv_file_path)  # 默认情况下，pandas会使用第一行作为列标题\n",
    "questions = df['问题内容'].tolist()  # 标题是\"问题内容\"\n",
    "answers = df['答案内容'].tolist()  # 标题是\"答案内容\"\n",
    "\n",
    "# 获取每个问题的嵌入向量\n",
    "embeddings = [get_embedding(doc) for doc in questions]\n",
    "\n",
    "# 将嵌入向量转换为NumPy数组，以便进行相似性搜索\n",
    "embeddings_matrix = np.vstack(embeddings)\n",
    "\n",
    "# 使用sklearn的NearestNeighbors构建一个索引\n",
    "nbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(embeddings_matrix)\n",
    "\n",
    "# 查询功能，返回最相似的问题索引和答案\n",
    "def query(text, k=len(questions)):\n",
    "    query_embedding = get_embedding(text).reshape(1, -1)\n",
    "    distances, indices = nbrs.kneighbors(query_embedding, n_neighbors=k)\n",
    "    # 返回所有相似问题及其答案和距离\n",
    "    return [(index, answers[index], distance) for index, distance in zip(indices[0], distances[0])]\n",
    "\n",
    "# 用一个查询来测试\n",
    "query_text = \"可视化\"\n",
    "similar_questions = query(query_text)\n",
    "for rank, (index, answer, distance) in enumerate(similar_questions, start=1):\n",
    "    print(f\"Rank {rank}:\")\n",
    "    print(f\"Most similar question: {questions[index]}\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(f\"Distance: {distance}\")\n",
    "    print()  # 空行分隔各个结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "726c4f9c-f48d-4003-a3d6-b26c372940d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://gpt-proxy.jd.com/gateway/azure/chat/completions \"HTTP/1.1 200 \"\n",
      "INFO:__main__:API response received.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最相似的文档：文档一的内容\n"
     ]
    }
   ],
   "source": [
    "def combined_query_and_reply(query_text):\n",
    "    # 使用向量匹配找到最相似的文档\n",
    "    indices, distances = query(query_text)\n",
    "    most_similar_doc_index = indices[0][0]  # 获取最相似文档的索引\n",
    "    most_similar_doc = documents[most_similar_doc_index]  # 获取最相似的文档内容\n",
    "\n",
    "    # 准备要发送给大模型的信息\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"你是一个帮助者。\"},\n",
    "        {\"role\": \"user\", \"content\": f\"我想问一下与以下内容相关的信息：{query_text}\"},\n",
    "        {\"role\": \"assistant\", \"content\": f\"根据您的查询，最相似的文档是：{most_similar_doc}。以下是关于该文档的详细信息：\"}\n",
    "    ]\n",
    "\n",
    "    # 调用 OpenAI SDK 来获取回复\n",
    "    response = open_ai_sdk(\"gpt-4-1106-preview\", messages)\n",
    "    if response:\n",
    "        try:\n",
    "            # 检查 Choice 对象有哪些属性\n",
    "            choice = response.choices[0]\n",
    "            assistant_reply = choice.message  # 假设 message 是正确的属性\n",
    "\n",
    "            # 打印最相似的文档和聊天机器人的回复\n",
    "            print(f\"最相似的文档：{most_similar_doc}\")\n",
    "            # print(\"聊天机器人的回复：\", assistant_reply)  # 也先注释掉\n",
    "        except (AttributeError, IndexError) as e:\n",
    "            # 如果出现属性错误或索引错误，记录日志\n",
    "            logger.error(f\"Error accessing the response data: {e}\")\n",
    "    else:\n",
    "        print(\"未能从 OpenAI API 获取有效响应。\")\n",
    "\n",
    "# 测试组合查询和回复\n",
    "if __name__ == '__main__':\n",
    "    query_text = \"我想找有关文档内容的信息\"\n",
    "    combined_query_and_reply(query_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8754add7-62e2-47b4-8952-451f8b8adeaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
