{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "025bfdaa-fe08-4f7e-ab2f-fbcdebf1ef05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('无正则化MSE: 0.0',\n",
       " ['L1正则化（Lasso）alpha=0.01, MSE: 2.500000000000052e-05',\n",
       "  'L2正则化（Ridge）alpha=0.01, MSE: 2.0393593811379104e-06',\n",
       "  'L1正则化（Lasso）alpha=0.1, MSE: 0.0025000000000000014',\n",
       "  'L2正则化（Ridge）alpha=0.1, MSE: 0.00020263167893012514',\n",
       "  'L1正则化（Lasso）alpha=1, MSE: 0.25',\n",
       "  'L2正则化（Ridge）alpha=1, MSE: 0.01902497027348397',\n",
       "  'L1正则化（Lasso）alpha=10, MSE: 16.0',\n",
       "  'L2正则化（Ridge）alpha=10, MSE: 1.108033240997228',\n",
       "  'L1正则化（Lasso）alpha=100, MSE: 16.0',\n",
       "  'L2正则化（Ridge）alpha=100, MSE: 9.765625'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 修改后的数据\n",
    "X = np.array([1, 2, 3, 4, 5, 6, 7]).reshape(-1, 1)  # 示例数据，可根据实际情况修改\n",
    "Y = np.array([2, 4, 6, 8, 10, 12, 14])  # 示例数据，可根据实际情况修改\n",
    "\n",
    "# 尝试不同的alpha值\n",
    "alphas = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "# 无正则化的线性回归\n",
    "model = LinearRegression()\n",
    "model.fit(X, Y)\n",
    "y_pred = model.predict(X)\n",
    "mse = mean_squared_error(Y, y_pred)\n",
    "mse_result = f\"无正则化MSE: {mse}\"\n",
    "\n",
    "# L1正则化（Lasso）和L2正则化（Ridge）的MSE结果\n",
    "lasso_ridge_results = []\n",
    "for alpha in alphas:\n",
    "    # L1正则化（Lasso）\n",
    "    lasso = Lasso(alpha=alpha)\n",
    "    lasso.fit(X, Y)\n",
    "    y_pred_lasso = lasso.predict(X)\n",
    "    mse_lasso = mean_squared_error(Y, y_pred_lasso)\n",
    "    lasso_result = f\"L1正则化（Lasso）alpha={alpha}, MSE: {mse_lasso}\"\n",
    "\n",
    "    # L2正则化（Ridge）\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X, Y)\n",
    "    y_pred_ridge = ridge.predict(X)\n",
    "    mse_ridge = mean_squared_error(Y, y_pred_ridge)\n",
    "    ridge_result = f\"L2正则化（Ridge）alpha={alpha}, MSE: {mse_ridge}\"\n",
    "\n",
    "    lasso_ridge_results.extend([lasso_result, ridge_result])\n",
    "\n",
    "mse_result, lasso_ridge_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "932beeea-32de-4799-9699-0f183001ef80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "无正则化MSE: 1382653.0612244897\n",
      "L1正则化（Lasso）MSE: 1382653.061225489\n",
      "L2正则化（Ridge）MSE: 1382653.061255988\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 数据\n",
    "X = np.array([150, 200, 250, 300, 350, 400, 450]).reshape(-1, 1)\n",
    "Y = np.array([6450, 7450, 8450, 9450, 11450, 15450, 18450])\n",
    "\n",
    "# 无正则化的线性回归\n",
    "model = LinearRegression()\n",
    "model.fit(X, Y)\n",
    "y_pred = model.predict(X)\n",
    "mse = mean_squared_error(Y, y_pred)\n",
    "print(f\"无正则化MSE: {mse}\")\n",
    "\n",
    "# L1正则化（Lasso）\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X, Y)\n",
    "y_pred_lasso = lasso.predict(X)\n",
    "mse_lasso = mean_squared_error(Y, y_pred_lasso)\n",
    "print(f\"L1正则化（Lasso）MSE: {mse_lasso}\")\n",
    "\n",
    "# L2正则化（Ridge）\n",
    "ridge = Ridge(alpha=0.1)\n",
    "ridge.fit(X, Y)\n",
    "y_pred_ridge = ridge.predict(X)\n",
    "mse_ridge = mean_squared_error(Y, y_pred_ridge)\n",
    "print(f\"L2正则化（Ridge）MSE: {mse_ridge}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6c196c8d-5a63-459e-a14f-8d8f1cf3d45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1.7680132279228005\n",
      "Feature importances:\n",
      "[0.02117573 0.02931126 0.11610443 0.18711439 0.6462942 ]\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 示例数据，这里使用pandas的DataFrame来表示\n",
    "# 假设我们有一个具有两个特征和一个目标变量的数据集\n",
    "# 修改数据\n",
    "#创建一个简单的模拟数据集\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 5)  # 100个样本，5个特征\n",
    "y = 2 * X[:, 0] + 3 * X[:, 1] + 4 * X[:, 2] + 5 * X[:, 3] + 6 * X[:, 4] + np.random.normal(0, 1, 100)  # 生成目标值\n",
    "\n",
    "# 创建DataFrame\n",
    "df_simulated = pd.DataFrame(X, columns=['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5'])\n",
    "df_simulated['Target'] = y\n",
    "\n",
    "# 显示数据集的前几行\n",
    "df_simulated.head()\n",
    "\n",
    "# 将数据集分为训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_simulated.drop('Target', axis=1), df_simulated['Target'], test_size=0.2, random_state=42)\n",
    "\n",
    "# 初始化XGBRegressor模型\n",
    "model = XGBRegressor(\n",
    "    objective='reg:squarederror',  # 回归任务的目标函数\n",
    "    learning_rate=0.1,             # 学习率\n",
    "    n_estimators=20,              # 树的数量\n",
    "    max_depth=5,                  # 树的最大深度\n",
    "    seed=42                        # 随机数种子\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 预测测试集\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 计算均方误差\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# 输出特征的重要性\n",
    "print(\"Feature importances:\")\n",
    "print(model.feature_importances_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "00ea327a-5a33-4ab7-9a41-d416ecbe9f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error for each fold: [3.80609426 2.25416231 3.19748597 2.56150698 4.47198249 4.94266164\n",
      " 2.90918719 2.35616686 3.45059807 3.8093679 ]\n",
      "Average Mean Squared Error: 3.375921366403788\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# 进行交叉验证\n",
    "scores = cross_val_score(model, df_simulated.drop('Target', axis=1), df_simulated['Target'], \n",
    "                         scoring='neg_mean_squared_error', cv=10)\n",
    "\n",
    "# 输出交叉验证的结果\n",
    "mse_scores = -scores\n",
    "print(f\"Mean Squared Error for each fold: {mse_scores}\")\n",
    "print(f\"Average Mean Squared Error: {mse_scores.mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d61ae3d0-1f50-42c8-a923-5b0fec05cccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'params': {'learning_rate': 0.1, 'n_estimators': 10, 'max_depth': 3},\n",
       "  'mse': 3.572566432460038,\n",
       "  'feature_importances': array([0.03654236, 0.06674168, 0.11232027, 0.152837  , 0.6315587 ],\n",
       "        dtype=float32)},\n",
       " {'params': {'learning_rate': 0.1, 'n_estimators': 20, 'max_depth': 5},\n",
       "  'mse': 1.7680132279228005,\n",
       "  'feature_importances': array([0.02117573, 0.02931126, 0.11610443, 0.18711439, 0.6462942 ],\n",
       "        dtype=float32)},\n",
       " {'params': {'learning_rate': 0.1, 'n_estimators': 30, 'max_depth': 7},\n",
       "  'mse': 2.1193700421836605,\n",
       "  'feature_importances': array([0.01522161, 0.03322139, 0.12095184, 0.22702113, 0.603584  ],\n",
       "        dtype=float32)},\n",
       " {'params': {'learning_rate': 0.3, 'n_estimators': 10, 'max_depth': 3},\n",
       "  'mse': 1.8016617908163362,\n",
       "  'feature_importances': array([0.03334754, 0.08151796, 0.12206134, 0.19867064, 0.56440246],\n",
       "        dtype=float32)},\n",
       " {'params': {'learning_rate': 0.3, 'n_estimators': 20, 'max_depth': 5},\n",
       "  'mse': 1.8116903355727723,\n",
       "  'feature_importances': array([0.02176036, 0.05783607, 0.12746783, 0.2420014 , 0.5509343 ],\n",
       "        dtype=float32)},\n",
       " {'params': {'learning_rate': 0.3, 'n_estimators': 30, 'max_depth': 7},\n",
       "  'mse': 2.3025081284806994,\n",
       "  'feature_importances': array([0.0177609 , 0.0407331 , 0.13277192, 0.28577533, 0.5229587 ],\n",
       "        dtype=float32)},\n",
       " {'params': {'learning_rate': 0.5, 'n_estimators': 10, 'max_depth': 3},\n",
       "  'mse': 2.4086447107100235,\n",
       "  'feature_importances': array([0.04019788, 0.07160161, 0.13724378, 0.20663093, 0.5443258 ],\n",
       "        dtype=float32)},\n",
       " {'params': {'learning_rate': 0.5, 'n_estimators': 20, 'max_depth': 5},\n",
       "  'mse': 2.685810016907999,\n",
       "  'feature_importances': array([0.01905414, 0.04243478, 0.16448583, 0.25003618, 0.5239891 ],\n",
       "        dtype=float32)},\n",
       " {'params': {'learning_rate': 0.5, 'n_estimators': 30, 'max_depth': 7},\n",
       "  'mse': 2.4890655481566255,\n",
       "  'feature_importances': array([0.01957076, 0.03641792, 0.12273414, 0.22550547, 0.59577173],\n",
       "        dtype=float32)}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义一个函数来训练模型并返回MSE和特征重要性\n",
    "def train_xgboost(X_train, X_test, y_train, y_test, params):\n",
    "    model = XGBRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    feature_importances = model.feature_importances_\n",
    "    return mse, feature_importances\n",
    "\n",
    "# 参数组合\n",
    "param_combinations = [\n",
    "    {'learning_rate': 0.1, 'n_estimators': 10, 'max_depth': 3},\n",
    "    {'learning_rate': 0.1, 'n_estimators': 20, 'max_depth': 5},\n",
    "    {'learning_rate': 0.1, 'n_estimators': 30, 'max_depth': 7},\n",
    "    {'learning_rate': 0.3, 'n_estimators': 10, 'max_depth': 3},\n",
    "    {'learning_rate': 0.3, 'n_estimators': 20, 'max_depth': 5},\n",
    "    {'learning_rate': 0.3, 'n_estimators': 30, 'max_depth': 7},\n",
    "    {'learning_rate': 0.5, 'n_estimators': 10, 'max_depth': 3},\n",
    "    {'learning_rate': 0.5, 'n_estimators': 20, 'max_depth': 5},\n",
    "    {'learning_rate': 0.5, 'n_estimators': 30, 'max_depth': 7},\n",
    "]\n",
    "\n",
    "# 记录结果\n",
    "results = []\n",
    "\n",
    "# 对每个参数组合进行训练并记录结果\n",
    "for params in param_combinations:\n",
    "    mse, feature_importances = train_xgboost(X_train, X_test, y_train, y_test, params)\n",
    "    results.append({'params': params, 'mse': mse, 'feature_importances': feature_importances})\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33699f2-4b8f-4b98-a9ed-03ae08588ede",
   "metadata": {},
   "source": [
    "## 特征重要性可视化 （回归）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "53b7575e-0bcf-4fdd-b3d9-e62c716b9ca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        <table class=\"eli5-weights eli5-feature-importances\" style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto;\">\n",
       "    <thead>\n",
       "    <tr style=\"border: none;\">\n",
       "        <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">Weight</th>\n",
       "        <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "    </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.8705\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Feature1\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 94.93%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.1224\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Feature3\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(120, 100.00%, 99.31%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0.0071\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                Feature2\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "    \n",
       "    </tbody>\n",
       "</table>\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "        <br>\n",
       "        <pre>Feature1 <= 375.000  (60.0%)\n",
       "    Feature3 <= 0.476  (40.0%)\n",
       "        Feature2 <= 0.582  (20.0%)  ---> 8450.0\n",
       "        Feature2 > 0.582  (20.0%)  ---> 9450.0\n",
       "    Feature3 > 0.476  (20.0%)  ---> 11450.0\n",
       "Feature1 > 375.000  (40.0%)\n",
       "    Feature3 <= 0.681  (20.0%)  ---> 18450.0\n",
       "    Feature3 > 0.681  (20.0%)  ---> 15450.0</pre>\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import eli5\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# 原始数据只有一个特征，我们将添加两个额外的特征\n",
    "area = np.array([150, 200, 250, 300, 350, 400, 450]).reshape(-1, 1)\n",
    "feature2 = np.random.rand(7).reshape(-1, 1)  # 随机生成的第二个特征\n",
    "feature3 = np.random.rand(7).reshape(-1, 1)  # 随机生成的第三个特征\n",
    "\n",
    "# 合并特征以形成3D数据集\n",
    "X = np.hstack((area, feature2, feature3))\n",
    "y = np.array([6450, 7450, 8450, 9450, 11450, 15450, 18450])\n",
    "\n",
    "# 划分数据集为训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 训练决策树回归器\n",
    "regressor = DecisionTreeRegressor(random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "# 预测测试集\n",
    "y_pred = regressor.predict(X_test)\n",
    "# 使用eli5来显示特征的重要性\n",
    "eli5.show_weights(regressor, feature_names=['Feature1', 'Feature2', 'Feature3'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0c5c2562-45b5-48cb-b9ea-752cf3544343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "    \n",
       "\n",
       "        \n",
       "            \n",
       "                \n",
       "                \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y\n",
       "    \n",
       "</b>\n",
       "\n",
       "    \n",
       "    (score <b>9450.000</b>)\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
       "                    Contribution<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +12650.000\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        &lt;BIAS&gt;\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 97.92%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +500.000\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        Feature2\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 97.02%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -833.333\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        Feature3\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 92.92%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -2866.667\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        Feature1\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "            \n",
       "        \n",
       "\n",
       "        \n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用eli5来显示单个预测的详细信息\n",
    "test_sample = X_test[0]\n",
    "eli5.show_prediction(regressor, test_sample, feature_names=['Feature1', 'Feature2', 'Feature3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bddcedff-6596-4184-9d39-2b83d5ce0b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 12500000.0\n"
     ]
    }
   ],
   "source": [
    "# 计算均方误差\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3775c19e-19b4-4810-919a-1cbb9ce987cd",
   "metadata": {},
   "source": [
    "## 卡方检验\n",
    "卡方检验（Chi-Square Test）是统计学中的一种常用假设检验方法，主要用于检验两个分类变量之间的独立性。\n",
    "\n",
    "卡方检验要求每个单元格的期望频数至少为5，否则可能需要使用连续性校正或者合并类别。\r\n",
    "卡方检验适用于分类数据，不适用于连续数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05d144d2-3b4f-42c5-a149-ad1415b96984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征: Feature1, 卡方统计量: 8.0, p值: 0.018315638888734182, 自由度: 2, 临界值: 5.991464547107979\n",
      "特征: Feature2, 卡方统计量: 0.0, p值: 1.0, 自由度: 2, 临界值: 5.991464547107979\n",
      "特征: Feature3, 卡方统计量: 0.3333333333333333, p值: 0.5637028616507731, 自由度: 1, 临界值: 3.841458820694124\n",
      "特征: Feature4, 卡方统计量: 2.0, p值: 0.36787944117144245, 自由度: 2, 临界值: 5.991464547107979\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Feature1']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import chi2\n",
    "\n",
    "# 假设数据集如下：\n",
    "data = {\n",
    "    'Feature1': [1, 2, 1, 2, 1, 2, 1, 2, 3, 3, 3, 3],\n",
    "    'Feature2': ['A', 'A', 'B', 'B', 'C', 'C', 'A', 'A', 'B', 'B', 'C', 'C'],\n",
    "    'Feature3': [True, False, True, False, True, False, True, False, True, False, True, False],\n",
    "    'Feature4': ['High', 'Low', 'High', 'Low', 'Medium', 'Medium', 'High', 'Low', 'Medium', 'Medium', 'High', 'Low'],\n",
    "    'Target': [1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1]\n",
    "}\n",
    "\n",
    "# 将数据转换为DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 使用 chi2 分布对象来计算临界值\n",
    "chi2_distribution = chi2\n",
    "\n",
    "# 创建一个空列表来存储每个特征的卡方统计量、p值和自由度\n",
    "chi2_results = []\n",
    "\n",
    "# 对每个特征进行卡方检验\n",
    "for feature in ['Feature1', 'Feature2', 'Feature3', 'Feature4']:\n",
    "    # 计算每个特征与目标变量的交叉表\n",
    "    contingency_table = pd.crosstab(df[feature], df['Target'])\n",
    "    # 执行卡方检验\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "    # 将结果添加到列表中\n",
    "    chi2_results.append((feature, chi2, p, dof))\n",
    "\n",
    "# 计算每个特征的卡方临界值\n",
    "chi2_critical_values = {}\n",
    "for feature, _, _, dof in chi2_results:\n",
    "    chi2_critical_value = chi2_distribution.ppf(1 - 0.05, dof)\n",
    "    chi2_critical_values[feature] = chi2_critical_value\n",
    "\n",
    "# 打印每个特征的卡方统计量、p值、自由度和临界值\n",
    "for feature, chi2, p, dof in chi2_results:\n",
    "    print(f\"特征: {feature}, 卡方统计量: {chi2}, p值: {p}, 自由度: {dof}, 临界值: {chi2_critical_values[feature]}\")\n",
    "\n",
    "# 选择p值小于显著性水平（例如0.05）的特征\n",
    "selected_features = [feature for feature, chi2, p, dof in chi2_results if p < 0.05]\n",
    "\n",
    "# 打印选择的特征\n",
    "selected_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d55ccbe-743e-4cbb-9e81-8db256e2a63d",
   "metadata": {},
   "source": [
    "## 互信息\n",
    "互信息（Mutual Information, MI）是一种衡量两个变量之间相互依赖性的量度。在特征选择中，互信息可以用来评估特征与目标变量之间的相关性。特征与目标变量之间的互信息越高，表示它们之间的依赖性越强，即该特征对目标变量的预测能力越强。\r\n",
    "\r\n",
    "在Python中，我们可以使用sklearn.metrics模块中的mutual_info_classif函数来计算特征与目标变量之间的互信息。以下是如何计算特征与目标变量之间的互信息并选择具有较高互信息的特征的步骤：\r\n",
    "\r\n",
    "使用mutual_info_classif计算每个特征与目标变量之间的互信息。\r\n",
    "根据互信息值对特征进行排序。\r\n",
    "选择具有最高互信息值的特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318201dc-1922-439f-90a7-8f8d69486939",
   "metadata": {},
   "source": [
    "互信息和卡方检验是两种不同的统计方法，它们在特征选择和数据分析中有各自的应用场景：\n",
    "### 互信息（Mutual Information）\n",
    "互信息是衡量两个变量之间相互依赖性的量。它适用于以下场景：\n",
    "1. **非参数环境**：互信息不需要假设数据遵循特定的分布，因此适用于非参数或非高斯分布的数据。\n",
    "2. **离散数据**：互信息主要用于处理离散变量。对于连续变量，需要先进行离散化处理。\n",
    "3. **特征选择**：在机器学习中，互信息可以用来评估特征与目标变量之间的相关性，尤其是在特征和目标变量都是分类变量时。\n",
    "4. **关联规则学习**：在市场篮子分析等场景中，互信息可以用来发现不同商品之间的关联性。\n",
    "5. **信息论应用**：在通信和信息论中，互信息用于衡量通信系统的有效性。\n",
    "### 卡方检验（Chi-Square Test）\n",
    "卡方检验是一种统计假设检验方法，适用于以下场景：\n",
    "1. **独立性检验**：最常用于检验两个分类变量之间是否独立。例如，检验性别与购买偏好之间是否独立。\n",
    "2. **拟合优度检验**：用于检验观察频数分布是否符合某个特定的分布。\n",
    "3. **特征选择**：在特征选择中，卡方检验可以用来评估分类特征与目标变量之间的关联性。如果卡方检验结果表明两个变量是独立的，那么这个特征可能对模型的预测能力贡献不大。\n",
    "4. **分类数据**：卡方检验主要适用于分类数据（名义或有序），不适用于连续数据。如果数据是连续的，需要先将其离散化。\n",
    "5. **大样本数据**：卡方检验要求样本量足够大，以确保分布近似于卡方分布。通常建议每个单元格的期望频数至少为5。\n",
    "总的来说，互信息和卡方检验都是用于评估变量之间关联性的方法，但它们适用的数据类型和场景有所不同。互信息适用于任何类型的数据，而卡方检验主要适用于分类数据，并且在实际应用中需要满足一定的样本量要求。在选择使用哪种方法时，应根据数据的特性和分析的目的来决定。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "651c0109-0f30-4781-9795-6439380d5b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Feature1', 0.9446789321789321),\n",
       " ('Feature3', 0.698647186147186),\n",
       " ('Feature4_High', 0.07860750360750379),\n",
       " ('Feature2_A', 0.0),\n",
       " ('Feature2_B', 0.0),\n",
       " ('Feature4_Low', 0.0),\n",
       " ('Feature4_Medium', 0.0)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# 对分类变量 Feature2 进行独热编码\n",
    "df_encoded = pd.get_dummies(df, columns=['Feature2', 'Feature4'])\n",
    "\n",
    "# 重新计算特征与目标变量之间的互信息\n",
    "mi = mutual_info_classif(df_encoded[['Feature1', 'Feature2_A', 'Feature2_B', 'Feature3', 'Feature4_Low', 'Feature4_Medium', 'Feature4_High']], df_encoded['Target'])\n",
    "\n",
    "\n",
    "## 将互信息与特征名称合并，并按互信息降序排序\n",
    "mi_features = sorted(zip(['Feature1', 'Feature2_A', 'Feature2_B', 'Feature3', 'Feature4_Low', 'Feature4_Medium', 'Feature4_High'], mi), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "mi_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ead3c9-cadd-4d75-857d-ecc6c276fce7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477a2490-f9f9-4a9b-928e-baebc0d9c63b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
